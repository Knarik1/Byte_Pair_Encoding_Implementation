{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0e1311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''After pre-tokenization, a set of unique words has been created and the frequency\n",
    "of each word it occurred in the training data has been determined. Next, BPE creates a base vocabulary \n",
    "consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from\n",
    "two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. \n",
    "Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer. As can be\n",
    "seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and punctuation \n",
    "tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined as \n",
    "splitting sentences into words. While it’s the most intuitive way to split texts into smaller chunks, this \n",
    "tokenization method can lead to problems for massive text corpora. In this case, space and punctuation\n",
    "tokenization usually generates a very big vocabulary. In contrast to BPE or WordPiece, Unigram initializes \n",
    "its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a \n",
    "smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most \n",
    "common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in \n",
    "conjunction '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18b3787b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '’'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0 = set(text.lower())\n",
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "157557f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPE(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(' ', '*')\n",
    "    \n",
    "    # do we count whitespaces also?\n",
    "    v0 = set(text)\n",
    "    \n",
    "    for k in range(300):\n",
    "        max_count_pair = ''\n",
    "        max_count = 0\n",
    "        max_char_1 = ''\n",
    "        max_char_2 = ''\n",
    "    \n",
    "        for char1 in v0:\n",
    "            for char2 in v0:\n",
    "                if not char1.endswith('*'):\n",
    "                    byte = char1 + char2\n",
    "\n",
    "                    if byte not in v0:\n",
    "                        pair_count = text.count(byte)\n",
    "\n",
    "                        # if equal what to do?\n",
    "                        if pair_count > max_count:\n",
    "                            max_count = pair_count\n",
    "                            max_count_pair = byte\n",
    "                            max_char_1 = char1\n",
    "                            max_char_2 = char2\n",
    "                    \n",
    "        print(max_count_pair, max_count, max_char_1, max_char_2)         \n",
    "        v0.add(max_count_pair)        \n",
    "   \n",
    "    return v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78288784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e* 35 e *\n",
      "s* 34 s *\n",
      "d* 26 d *\n",
      "in 23 i n\n",
      "n* 21 n *\n",
      "to 20 t o\n",
      "th 19 t h\n",
      "at 18 a t\n",
      "ti 17 t i\n",
      "en 17 e n\n",
      "or 17 o r\n",
      "on 17 o n\n",
      "ni 17 n i\n",
      "as 16 a s\n",
      "se 15 s e\n",
      "he 14 h e\n",
      "re 14 r e\n",
      "ar 13 a r\n",
      "an 13 a n\n",
      "the 13 th e\n",
      "the* 13 th e*\n",
      "t* 13 t *\n",
      "he* 13 he *\n",
      "y* 13 y *\n",
      "iz 13 i z\n",
      "ul 13 u l\n",
      "ed 13 e d\n",
      "er 13 e r\n",
      "ed* 12 ed *\n",
      "io 12 i o\n",
      "tio 12 t io\n",
      "ion 12 i on\n",
      "tion 12 t ion\n",
      "o* 12 o *\n",
      "ca 12 c a\n",
      "ati 11 a ti\n",
      "atio 11 a tio\n",
      "ation 11 atio n\n",
      "es 11 e s\n",
      "oc 11 o c\n",
      "niz 10 ni z\n",
      "la 10 l a\n",
      "lar 10 l ar\n",
      "le 10 l e\n",
      "te 10 t e\n",
      "eniz 10 en iz\n",
      "eni 10 en i\n",
      "ok 10 o k\n",
      "un 10 u n\n",
      "ken 10 k en\n",
      "oken 10 o ken\n",
      "keni 10 k eni\n",
      "okeni 10 o keni\n",
      "keniz 10 keni z\n",
      "okeniz 10 o keniz\n",
      "ke 10 k e\n",
      "oke 10 o ke\n",
      "tokeniz 10 to keniz\n",
      "tokeni 10 to keni\n",
      "tok 10 to k\n",
      "toke 10 to ke\n",
      "token 10 toke n\n",
      "to* 10 to *\n",
      "bu 10 b u\n",
      ".* 10 . *\n",
      "ry 10 r y\n",
      "r* 10 r *\n",
      "ary 9 ar y\n",
      "lary 9 la ry\n",
      "oca 9 oc a\n",
      "nd* 9 n d*\n",
      "nd 9 n d\n",
      "ab 9 a b\n",
      "ocab 9 oc ab\n",
      "abu 9 ab u\n",
      "ocabu 9 oc abu\n",
      "abul 9 ab ul\n",
      "ocabul 9 oc abul\n",
      "ocabular 9 ocabul ar\n",
      "ocabula 9 ocabul a\n",
      "ocabulary 9 ocabul ary\n",
      "abula 9 abu la\n",
      "abular 9 abu lar\n",
      "abulary 9 abu lary\n",
      "vocabul 9 v ocabul\n",
      "vocabular 9 vocabul ar\n",
      "vocabula 9 vocabul a\n",
      "vocabulary 9 vocabul ary\n",
      "voc 9 v oc\n",
      "vocab 9 voc ab\n",
      "vocabu 9 vocab u\n",
      "voca 9 voc a\n",
      "vo 9 v o\n",
      "ula 9 u la\n",
      "ulary 9 ula ry\n",
      "ular 9 ula r\n",
      "bulary 9 b ulary\n",
      "bula 9 b ula\n",
      "bul 9 b ul\n",
      "bular 9 bul ar\n",
      "cabular 9 ca bular\n",
      "cabulary 9 cabular y\n",
      "cabul 9 ca bul\n",
      "cabula 9 cabul a\n",
      "cab 9 ca b\n",
      "cabu 9 ca bu\n",
      "it 9 i t\n",
      "ra 9 r a\n",
      ",* 9 , *\n",
      "ization 8 iz ation\n",
      "izat 8 iz at\n",
      "iza 8 iz a\n",
      "izati 8 iz ati\n",
      "izatio 8 iz atio\n",
      "wo 8 w o\n",
      "nization 8 niz ation\n",
      "nizat 8 niz at\n",
      "niza 8 niz a\n",
      "nizati 8 niz ati\n",
      "nizatio 8 niz atio\n",
      "enizat 8 en izat\n",
      "eniza 8 en iza\n",
      "enizatio 8 en izatio\n",
      "enizati 8 en izati\n",
      "enization 8 en ization\n",
      "of 8 o f\n",
      "of* 8 of *\n",
      "on* 8 o n*\n",
      "okenization 8 oke nization\n",
      "okenizati 8 oke nizati\n",
      "okeniza 8 oke niza\n",
      "okenizatio 8 oke nizatio\n",
      "okenizat 8 oke nizat\n",
      "f* 8 f *\n",
      "and 8 a nd\n",
      "a* 8 a *\n",
      "and* 8 a nd*\n",
      "kenization 8 keniz ation\n",
      "kenizat 8 keniz at\n",
      "keniza 8 keniz a\n",
      "kenizati 8 keniz ati\n",
      "kenizatio 8 keniz atio\n",
      "tokenization 8 tokeniz ation\n",
      "tokenizat 8 tokeniz at\n",
      "tokeniza 8 tokeniz a\n",
      "tokenizati 8 tokeniz ati\n",
      "tokenizatio 8 tokeniz atio\n",
      "zation 8 z ation\n",
      "zat 8 z at\n",
      "zatio 8 zat io\n",
      "zati 8 zat i\n",
      "za 8 z a\n",
      "wor 7 w or\n",
      "es* 7 es *\n",
      "ion* 7 ion *\n",
      "ne 7 n e\n",
      "nc 7 n c\n",
      "ase 7 a se\n",
      "as* 7 a s*\n",
      "ry* 7 ry *\n",
      "tion* 7 ti on*\n",
      "in* 7 in *\n",
      "word 7 wor d\n",
      "ord 7 or d\n",
      "is 7 i s\n",
      "rd 7 r d\n",
      "si 7 s i\n",
      "co 7 c o\n",
      "vocabulary* 6 vocabulary *\n",
      "ary* 6 ar y*\n",
      "ocabulary* 6 ocabul ary*\n",
      "bulary* 6 bular y*\n",
      "lary* 6 la ry*\n",
      "cabulary* 6 cabu lary*\n",
      "ulary* 6 ulary *\n",
      "abulary* 6 ab ulary*\n",
      "ation* 6 ation *\n",
      "nt 6 n t\n",
      "fo 6 f o\n",
      "for 6 f or\n",
      "al 6 a l\n",
      "is* 6 is *\n",
      "ea 6 e a\n",
      "mb 6 m b\n",
      "base 6 b ase\n",
      "bo 6 b o\n",
      "ba 6 b a\n",
      "bas 6 ba s\n",
      "ll 6 l l\n",
      "l* 6 l *\n",
      "tr 6 t r\n",
      "de 6 d e\n",
      "sp 6 s p\n",
      "st 6 s t\n",
      "ce 6 c e\n",
      "ize 5 iz e\n",
      "ha 5 h a\n",
      "ol 5 o l\n",
      "ng 5 n g\n",
      "all 5 a ll\n",
      "ac 5 a c\n",
      "bol 5 bo l\n",
      "er* 5 e r*\n",
      "et 5 e t\n",
      "ing 5 in g\n",
      "mbo 5 mb o\n",
      "mbol 5 mbo l\n",
      "g* 5 g *\n",
      "sed* 5 se d*\n",
      "sed 5 se d\n",
      "be 5 b e\n",
      "ymbol 5 y mbol\n",
      "ymbo 5 y mbo\n",
      "ymb 5 y mb\n",
      "ym 5 y m\n",
      "ze 5 z e\n",
      "sym 5 s ym\n",
      "symbo 5 sym bo\n",
      "symbol 5 symbo l\n",
      "symb 5 sym b\n",
      "sy 5 s y\n",
      "ct 5 c t\n",
      "ization* 4 iz ation*\n",
      "enization,* 4 enization ,*\n",
      "enization* 4 enization *\n",
      "enization, 4 enization ,\n",
      "ase* 4 ase *\n",
      "nization* 4 niz ation*\n",
      "h* 4 h *\n",
      "hi 4 h i\n",
      "tra 4 tr a\n",
      "words 4 word s\n",
      "tokenization,* 4 tokenization ,*\n",
      "tokenization* 4 tokenization *\n",
      "tokenization, 4 tokenization ,\n",
      "or* 4 o r*\n",
      "ion,* 4 ion ,*\n",
      "ization,* 4 izat ion,*\n",
      "ion, 4 ion ,\n",
      "ization, 4 izat ion,\n",
      "ation,* 4 ation ,*\n",
      "nization,* 4 niz ation,*\n",
      "ation, 4 ation ,\n",
      "nization, 4 niz ation,\n",
      "nct 4 n ct\n",
      "n,* 4 n ,*\n",
      "on,* 4 o n,*\n",
      "ng* 4 n g*\n",
      "n, 4 n ,\n",
      "on, 4 o n,\n",
      "ns 4 n s\n",
      "okenization, 4 oke nization,\n",
      "okenization* 4 oke nization*\n",
      "okenization,* 4 oke nization,*\n",
      "ain 4 a in\n",
      "am 4 a m\n",
      "ai 4 a i\n",
      "zation, 4 zatio n,\n",
      "zation* 4 zatio n*\n",
      "zation,* 4 zatio n,*\n",
      "rds 4 rd s\n",
      "ords 4 o rds\n",
      "kenization, 4 kenizatio n,\n",
      "kenization* 4 kenizatio n*\n",
      "kenization,* 4 kenizatio n,*\n",
      "ve 4 v e\n",
      "uni 4 u ni\n",
      "ua 4 u a\n",
      "unct 4 u nct\n",
      "unc 4 u nc\n",
      "us 4 u s\n",
      "ex 4 e x\n",
      "el 4 e l\n",
      "e- 4 e -\n",
      "tion,* 4 ti on,*\n",
      "ing* 4 in g*\n",
      "ine 4 in e\n",
      "mo 4 m o\n",
      "me 4 m e\n",
      "ta 4 t a\n",
      "tu 4 t u\n",
      "tion, 4 t ion,\n",
      "base* 4 base *\n",
      "ly* 4 l y*\n",
      "ls 4 l s\n",
      "ls* 4 l s*\n",
      "ly 4 l y\n",
      "ce* 4 ce *\n",
      "ch 4 c h\n",
      "\n",
      "t 4 \n",
      " t\n",
      "pr 4 p r\n",
      "pa 4 p a\n",
      "ds 4 d s\n",
      "se* 4 s e*\n",
      "xt 3 x t\n",
      "ymbols 3 ymbol s\n",
      "ymbols* 3 ymbol s*\n",
      "used* 3 us ed*\n",
      "use 3 us e\n",
      "used 3 us ed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " '\\nt',\n",
       " '*',\n",
       " ',',\n",
       " ',*',\n",
       " '-',\n",
       " '.',\n",
       " '.*',\n",
       " 'a',\n",
       " 'a*',\n",
       " 'ab',\n",
       " 'abu',\n",
       " 'abul',\n",
       " 'abula',\n",
       " 'abular',\n",
       " 'abulary',\n",
       " 'abulary*',\n",
       " 'ac',\n",
       " 'ai',\n",
       " 'ain',\n",
       " 'al',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'and*',\n",
       " 'ar',\n",
       " 'ary',\n",
       " 'ary*',\n",
       " 'as',\n",
       " 'as*',\n",
       " 'ase',\n",
       " 'ase*',\n",
       " 'at',\n",
       " 'ati',\n",
       " 'atio',\n",
       " 'ation',\n",
       " 'ation*',\n",
       " 'ation,',\n",
       " 'ation,*',\n",
       " 'b',\n",
       " 'ba',\n",
       " 'bas',\n",
       " 'base',\n",
       " 'base*',\n",
       " 'be',\n",
       " 'bo',\n",
       " 'bol',\n",
       " 'bu',\n",
       " 'bul',\n",
       " 'bula',\n",
       " 'bular',\n",
       " 'bulary',\n",
       " 'bulary*',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cabu',\n",
       " 'cabul',\n",
       " 'cabula',\n",
       " 'cabular',\n",
       " 'cabulary',\n",
       " 'cabulary*',\n",
       " 'ce',\n",
       " 'ce*',\n",
       " 'ch',\n",
       " 'co',\n",
       " 'ct',\n",
       " 'd',\n",
       " 'd*',\n",
       " 'de',\n",
       " 'ds',\n",
       " 'e',\n",
       " 'e*',\n",
       " 'e-',\n",
       " 'ea',\n",
       " 'ed',\n",
       " 'ed*',\n",
       " 'el',\n",
       " 'en',\n",
       " 'eni',\n",
       " 'eniz',\n",
       " 'eniza',\n",
       " 'enizat',\n",
       " 'enizati',\n",
       " 'enizatio',\n",
       " 'enization',\n",
       " 'enization*',\n",
       " 'enization,',\n",
       " 'enization,*',\n",
       " 'er',\n",
       " 'er*',\n",
       " 'es',\n",
       " 'es*',\n",
       " 'et',\n",
       " 'ex',\n",
       " 'f',\n",
       " 'f*',\n",
       " 'fo',\n",
       " 'for',\n",
       " 'g',\n",
       " 'g*',\n",
       " 'h',\n",
       " 'h*',\n",
       " 'ha',\n",
       " 'he',\n",
       " 'he*',\n",
       " 'hi',\n",
       " 'i',\n",
       " 'in',\n",
       " 'in*',\n",
       " 'ine',\n",
       " 'ing',\n",
       " 'ing*',\n",
       " 'io',\n",
       " 'ion',\n",
       " 'ion*',\n",
       " 'ion,',\n",
       " 'ion,*',\n",
       " 'is',\n",
       " 'is*',\n",
       " 'it',\n",
       " 'iz',\n",
       " 'iza',\n",
       " 'izat',\n",
       " 'izati',\n",
       " 'izatio',\n",
       " 'ization',\n",
       " 'ization*',\n",
       " 'ization,',\n",
       " 'ization,*',\n",
       " 'ize',\n",
       " 'j',\n",
       " 'k',\n",
       " 'ke',\n",
       " 'ken',\n",
       " 'keni',\n",
       " 'keniz',\n",
       " 'keniza',\n",
       " 'kenizat',\n",
       " 'kenizati',\n",
       " 'kenizatio',\n",
       " 'kenization',\n",
       " 'kenization*',\n",
       " 'kenization,',\n",
       " 'kenization,*',\n",
       " 'l',\n",
       " 'l*',\n",
       " 'la',\n",
       " 'lar',\n",
       " 'lary',\n",
       " 'lary*',\n",
       " 'le',\n",
       " 'll',\n",
       " 'ls',\n",
       " 'ls*',\n",
       " 'ly',\n",
       " 'ly*',\n",
       " 'm',\n",
       " 'mb',\n",
       " 'mbo',\n",
       " 'mbol',\n",
       " 'me',\n",
       " 'mo',\n",
       " 'n',\n",
       " 'n*',\n",
       " 'n,',\n",
       " 'n,*',\n",
       " 'nc',\n",
       " 'nct',\n",
       " 'nd',\n",
       " 'nd*',\n",
       " 'ne',\n",
       " 'ng',\n",
       " 'ng*',\n",
       " 'ni',\n",
       " 'niz',\n",
       " 'niza',\n",
       " 'nizat',\n",
       " 'nizati',\n",
       " 'nizatio',\n",
       " 'nization',\n",
       " 'nization*',\n",
       " 'nization,',\n",
       " 'nization,*',\n",
       " 'ns',\n",
       " 'nt',\n",
       " 'o',\n",
       " 'o*',\n",
       " 'oc',\n",
       " 'oca',\n",
       " 'ocab',\n",
       " 'ocabu',\n",
       " 'ocabul',\n",
       " 'ocabula',\n",
       " 'ocabular',\n",
       " 'ocabulary',\n",
       " 'ocabulary*',\n",
       " 'of',\n",
       " 'of*',\n",
       " 'ok',\n",
       " 'oke',\n",
       " 'oken',\n",
       " 'okeni',\n",
       " 'okeniz',\n",
       " 'okeniza',\n",
       " 'okenizat',\n",
       " 'okenizati',\n",
       " 'okenizatio',\n",
       " 'okenization',\n",
       " 'okenization*',\n",
       " 'okenization,',\n",
       " 'okenization,*',\n",
       " 'ol',\n",
       " 'on',\n",
       " 'on*',\n",
       " 'on,',\n",
       " 'on,*',\n",
       " 'or',\n",
       " 'or*',\n",
       " 'ord',\n",
       " 'ords',\n",
       " 'p',\n",
       " 'pa',\n",
       " 'pr',\n",
       " 'q',\n",
       " 'r',\n",
       " 'r*',\n",
       " 'ra',\n",
       " 'rd',\n",
       " 'rds',\n",
       " 're',\n",
       " 'ry',\n",
       " 'ry*',\n",
       " 's',\n",
       " 's*',\n",
       " 'se',\n",
       " 'se*',\n",
       " 'sed',\n",
       " 'sed*',\n",
       " 'si',\n",
       " 'sp',\n",
       " 'st',\n",
       " 'sy',\n",
       " 'sym',\n",
       " 'symb',\n",
       " 'symbo',\n",
       " 'symbol',\n",
       " 't',\n",
       " 't*',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'th',\n",
       " 'the',\n",
       " 'the*',\n",
       " 'ti',\n",
       " 'tio',\n",
       " 'tion',\n",
       " 'tion*',\n",
       " 'tion,',\n",
       " 'tion,*',\n",
       " 'to',\n",
       " 'to*',\n",
       " 'tok',\n",
       " 'toke',\n",
       " 'token',\n",
       " 'tokeni',\n",
       " 'tokeniz',\n",
       " 'tokeniza',\n",
       " 'tokenizat',\n",
       " 'tokenizati',\n",
       " 'tokenizatio',\n",
       " 'tokenization',\n",
       " 'tokenization*',\n",
       " 'tokenization,',\n",
       " 'tokenization,*',\n",
       " 'tr',\n",
       " 'tra',\n",
       " 'tu',\n",
       " 'u',\n",
       " 'ua',\n",
       " 'ul',\n",
       " 'ula',\n",
       " 'ular',\n",
       " 'ulary',\n",
       " 'ulary*',\n",
       " 'un',\n",
       " 'unc',\n",
       " 'unct',\n",
       " 'uni',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'used*',\n",
       " 'v',\n",
       " 've',\n",
       " 'vo',\n",
       " 'voc',\n",
       " 'voca',\n",
       " 'vocab',\n",
       " 'vocabu',\n",
       " 'vocabul',\n",
       " 'vocabula',\n",
       " 'vocabular',\n",
       " 'vocabulary',\n",
       " 'vocabulary*',\n",
       " 'w',\n",
       " 'wo',\n",
       " 'wor',\n",
       " 'word',\n",
       " 'words',\n",
       " 'x',\n",
       " 'xt',\n",
       " 'y',\n",
       " 'y*',\n",
       " 'ym',\n",
       " 'ymb',\n",
       " 'ymbo',\n",
       " 'ymbol',\n",
       " 'ymbols',\n",
       " 'ymbols*',\n",
       " 'z',\n",
       " 'za',\n",
       " 'zat',\n",
       " 'zati',\n",
       " 'zatio',\n",
       " 'zation',\n",
       " 'zation*',\n",
       " 'zation,',\n",
       " 'zation,*',\n",
       " 'ze',\n",
       " '’'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = BPE(text)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a146bab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b6198ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'It does so until the vocabulary has attained the desired vocabulary size.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97021d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPE_test(sentence, vocab):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(' ', '*')\n",
    "    \n",
    "    sentence_tokenized = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(sentence):\n",
    "        token = '<unk>'\n",
    "        \n",
    "        for end in range(start+1, len(sentence)+1):\n",
    "\n",
    "            if sentence[start:end] in vocab:\n",
    "                token = sentence[start:end]\n",
    "\n",
    "                if end == len(sentence):\n",
    "                    sentence_tokenized.append(token)\n",
    "                    start = end+1\n",
    "                    break\n",
    "            else:                \n",
    "                sentence_tokenized.append(token)\n",
    "                start = end-1\n",
    "                break\n",
    "                \n",
    "    formated = []\n",
    "\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] == \"*\":\n",
    "            formated[i-1] = formated[i-1] + \"*\"\n",
    "        else:\n",
    "            formated.append(tokenized[i])            \n",
    "                \n",
    "     \n",
    "    formated_2 = []\n",
    "\n",
    "    # don't use these punctuations -> # ' *\n",
    "    punct = '!\"$%&()+, -./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "    for i, byte in enumerate(formated):\n",
    "        if byte in punct:\n",
    "            formated_2.append(byte)\n",
    "        elif byte.endswith(\"*\"):\n",
    "            if i!=0 and not formated[i-1].endswith(\"*\"):\n",
    "                formated_2.append(\"##\" + byte[:-1])\n",
    "            else:    \n",
    "                formated_2.append(byte[:-1])\n",
    "        elif formated[i-1].endswith(\"*\"):\n",
    "            formated_2.append(byte)\n",
    "        else:\n",
    "            formated_2.append(\"##\" + byte)\n",
    "    \n",
    "\n",
    "                \n",
    "    return sentence_tokenized, formated_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "597b88ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " '*',\n",
       " 'd',\n",
       " 'o',\n",
       " 'es*',\n",
       " 's',\n",
       " 'o*',\n",
       " 'un',\n",
       " 'ti',\n",
       " 'l*',\n",
       " 'the*',\n",
       " 'vocabulary*',\n",
       " 'ha',\n",
       " 's*',\n",
       " 'at',\n",
       " 'ta',\n",
       " 'ine',\n",
       " 'd*',\n",
       " 'the*',\n",
       " 'de',\n",
       " 'si',\n",
       " 're',\n",
       " 'd*',\n",
       " 'vocabulary*',\n",
       " 'si',\n",
       " 'ze',\n",
       " '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized, tokenized_formated = BPE_test(test_sentence, vocabulary)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36dcd3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'd',\n",
       " '##o',\n",
       " '##es',\n",
       " 's',\n",
       " '##o',\n",
       " 'un',\n",
       " '##ti',\n",
       " '##l',\n",
       " 'the',\n",
       " 'vocabulary',\n",
       " 'ha',\n",
       " '##s',\n",
       " 'at',\n",
       " '##ta',\n",
       " '##ine',\n",
       " '##d',\n",
       " 'the',\n",
       " 'de',\n",
       " '##si',\n",
       " '##re',\n",
       " '##d',\n",
       " 'vocabulary',\n",
       " 'si',\n",
       " '##ze',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "exps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
